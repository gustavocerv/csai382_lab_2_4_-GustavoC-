{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30e635e-f65e-40db-b6cc-208cfd45f21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b5712e-9de5-499b-9246-6f2ac6217406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f245eee6-d184-49ef-a22a-2d9502e4fcad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Set up log file name with current date and time\n",
    "now = datetime.now()\n",
    "log_filename = f\"logs/run_{now.strftime('%Y%m%d_%H%M')}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(log_filename)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Log start of run\n",
    "logging.info(\"Run started.\")\n",
    "logging.info(f\"Cluster/runtime info: AWS Serverless interactive cluster (terminated by inactivity)\")\n",
    "\n",
    "# âœ… Updated paths to GitHub-connected Databricks repo\n",
    "logging.info(f\"menu file path: /Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/menu_items.csv\")\n",
    "logging.info(f\"orders file path: /Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/order_details.csv\")\n",
    "logging.info(f\"Log file: {log_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d90198-cc1f-445a-961c-b648f5794ba7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "import subprocess\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Fix random seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "logging.info(\"Random seeds set to 0\")\n",
    "\n",
    "# Capture environment\n",
    "!pip freeze > requirements.txt\n",
    "logging.info(\"Saved environment packages to requirements.txt\")\n",
    "\n",
    "# Compute SHA-256 hashes for input CSVs\n",
    "def compute_sha256(file_path):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "data_files = [\n",
    "    \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/menu_items.csv\",\n",
    "    \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/order_details.csv\"\n",
    "]\n",
    "\n",
    "hashes = {}\n",
    "for f in data_files:\n",
    "    if os.path.exists(f):\n",
    "        hashes[f] = compute_sha256(f)\n",
    "    else:\n",
    "        logging.warning(f\"File not found: {f}. Skipping hash computation.\")\n",
    "\n",
    "with open(\"data_hashes.json\", \"w\") as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "\n",
    "logging.info(f\"Data hashes saved to data_hashes.json: {hashes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a35eec-7372-4e48-8081-3da2e70871fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load menu_items and order_details CSVs"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "menu = pd.read_csv('/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/menu_items.csv')\n",
    "orders = pd.read_csv('/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/data_samples/order_details.csv')\n",
    "\n",
    "print('menu_items shape:', menu.shape)\n",
    "print('order_details shape:', orders.shape)\n",
    "display(menu.head())\n",
    "display(orders.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbfb7b7-1915-4473-a9a8-48cff5e2899c",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768962784053}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Top 5 item orders with details"
    }
   },
   "outputs": [],
   "source": [
    "# Clean basic issues\n",
    "orders['order_date'] = pd.to_datetime(orders['order_date'], errors='coerce')\n",
    "orders['order_time'] = orders['order_time'].str.strip()\n",
    "if 'item_id' in orders.columns:\n",
    "    orders['item_id'] = orders['item_id'].astype('Int64')\n",
    "\n",
    "menu['item_name'] = menu['item_name'].str.strip()\n",
    "menu['category'] = menu['category'].str.strip()\n",
    "\n",
    "# Join on menu_items.menu_item_id = order_details.item_id\n",
    "etl_df = orders.merge(menu, left_on='item_id', right_on='menu_item_id', how='left')\n",
    "\n",
    "# Create tidy table with useful columns\n",
    "etl_df = etl_df[['order_id', 'order_date', 'order_time', 'item_name', 'category', 'price']]\n",
    "display(etl_df.head())\n",
    "\n",
    "# Save cleaned and joined outputs (TOP 5 ONLY)\n",
    "import os\n",
    "output_dir = '/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "menu.to_csv(f'{output_dir}/menu_items_loaded.csv', index=False)\n",
    "orders.to_csv(f'{output_dir}/order_details_loaded.csv', index=False)\n",
    "\n",
    "# ðŸ”‘ Save only the top 5 rows\n",
    "etl_df.head(5).to_csv(f'{output_dir}/etl_df_cleaned_joined_top5.csv', index=False)\n",
    "\n",
    "print('Saved menu_items, order_details, and TOP 5 rows of cleaned/joined etl_df to etl_output directory.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e2bdd3-5942-4ad3-99eb-8fc81c35029a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ethical Reflection\n",
    "\n",
    "When working with data and AI, it is crucial to consider the ethical implications of our practices. Sensitive information, such as personal customer details or passwords, should never be written to log files, as logs may be accessible to others and could pose privacy risks. Ensuring reproducibility by capturing random seeds, environment details, and input file hashes supports accountability and fairnessâ€”allowing others to verify analyses and confirm consistent results. This is especially important when models or decisions impact individuals, as reproducible workflows help reduce errors and bias. By following these practices, we foster trust and transparency in data-driven projects, upholding ethical standards and protecting those affected by our work."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8359061630840856,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
