{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0965c6e2-f5b6-4a6b-8a1f-feff07159504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Load Pipeline and Transformed Data**\n",
    "The preprocessing pipeline and transformed datasets are loaded from saved .pkl files.\n",
    "A helper function ensures that the data is converted into a proper numeric 2-D matrix, which is required by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5754e7-cd9c-4f04-a0da-1ed361136365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n",
    "pipeline = joblib.load(\n",
    "   \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/stedi_feature_pipeline.pkl\"\n",
    ")\n",
    "X_train_transformed = joblib.load(\n",
    "   \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/X_train_transformed.pkl\"\n",
    ")\n",
    "X_test_transformed = joblib.load(\n",
    "   \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/X_test_transformed.pkl\"\n",
    ")\n",
    "def to_float_matrix(arr: np.ndarray) -> np.ndarray:\n",
    "   \"\"\"\n",
    "   Ensures that input arrays (possibly object-dtype, sparse, or 0-d) are converted to a 2-D float matrix.\n",
    "   This is necessary because saved feature arrays may have inconsistent shapes or types after transformation,\n",
    "   and ML models require numeric 2-D arrays for training and prediction.\n",
    "   \"\"\"\n",
    "   if arr.ndim == 0:\n",
    "       # Handle 0-d array directly\n",
    "       arr = arr.item()\n",
    "       if issparse(arr):\n",
    "           arr = arr.toarray()\n",
    "       arr = np.array(arr, dtype=float)\n",
    "   elif arr.dtype == object:\n",
    "       arr = np.array([\n",
    "           x.toarray() if issparse(x) else np.array(x, dtype=float)\n",
    "           for x in arr\n",
    "       ])\n",
    "       arr = np.vstack(arr)\n",
    "   elif issparse(arr):\n",
    "       arr = arr.toarray()\n",
    "   else:\n",
    "       arr = np.array(arr, dtype=float)\n",
    "   return arr\n",
    "X_train = to_float_matrix(X_train_transformed)\n",
    "X_test = to_float_matrix(X_test_transformed)\n",
    "y_train = joblib.load(\n",
    "   \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/y_train.pkl\"\n",
    ")\n",
    "y_test = joblib.load(\n",
    "   \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/y_test.pkl\"\n",
    ")\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7719593e-89f4-4035-83f4-38c5d537e770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Logistic Regression with Grid Search**\n",
    "A Logistic Regression model is tuned using GridSearchCV to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e88ab6-55fb-4c17-a282-4e07de15aaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg_params = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "log_reg_grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=300),\n",
    "    log_reg_params,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "\n",
    "log_reg_best_params = log_reg_grid.best_params_\n",
    "log_reg_best_score = log_reg_grid.best_score_\n",
    "\n",
    "log_reg_best_params, log_reg_best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6517d4-024f-44e1-82a0-397ec61134b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Random Forest with Grid Search**\n",
    "A Random Forest classifier is also tuned using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0290ed0-1843-4ecd-b175-0a820f1893ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    rf_params,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_best_params = rf_grid.best_params_\n",
    "rf_best_score = rf_grid.best_score_\n",
    "\n",
    "rf_best_params, rf_best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d84cc2d-7614-4c9d-9b2f-3ac5d9e63502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Model Selection**\n",
    "The two models are compared using their best cross-validation accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed24787-7da7-4e9d-9c30-ab3d524a8fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose the better model based on best_score_\n",
    "if rf_best_score > log_reg_best_score:\n",
    "    best_model = rf_grid.best_estimator_\n",
    "    best_model_name = \"Random Forest\"\n",
    "else:\n",
    "    best_model = log_reg_grid.best_estimator_\n",
    "    best_model_name = \"Logistic Regression\"\n",
    "\n",
    "best_model_name, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4179aa8-ff04-4bf8-b184-6a83da74d718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Save the Best Model**\n",
    "The selected model is saved for deployment or future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a778c653-a9e0-4b59-b958-7c89871726b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model = rf_grid.best_estimator_   # <-- replace with your winner\n",
    "joblib.dump(best_model, \"/Workspace/Users/gsc314@ensign.edu/csai382_lab_2_4_-GustavoC-/etl_pipeline/stedi_best_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f364dd53-8214-48e2-9e16-18d237043493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Model Evaluation Report and Ethics Reflection**\n",
    "\n",
    "After tuning both models, the Logistic Regression model performed best based on the cross-validation accuracy score. The comparison showed that Logistic Regression achieved a higher best score than the Random Forest model, which means it made more accurate predictions during validation. The main metric used for comparison was accuracy, because the grid search was configured to optimize this score. The hyperparameter that improved the Logistic Regression model was the regularization strength (C=0.01), which helped control overfitting and made the model more stable. One interesting result was that the simpler Logistic Regression model outperformed the more complex Random Forest model, suggesting that the dataset may have relatively simple patterns that do not require a highly complex model.\n",
    "\n",
    "If more time were available, the next step would be to test additional hyperparameters, try different models such as Gradient Boosting or Support Vector Machines, and evaluate performance using other metrics like precision, recall, and F1-score. This would provide a more complete understanding of the modelâ€™s behavior, especially if the dataset is imbalanced. However, hyperparameter tuning can accidentally introduce bias if the model becomes too optimized for patterns in the training data and performs poorly on underrepresented groups. This is why transparency is important. We should clearly report metrics, parameters, and model choices so others can understand and verify the results. Gospel principles also teach honesty and integrity. Just as we are taught to be truthful and fair in our actions, we should evaluate models honestly, report results accurately, and avoid hiding weaknesses or biases in our work."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.3 Trained ML Models: Hyperparameter Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
